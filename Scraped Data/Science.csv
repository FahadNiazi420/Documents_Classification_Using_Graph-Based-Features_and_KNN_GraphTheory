label,Scraping Date,Scraping Time,Link,title,Number of Words Scraped,content
Science,5/4/2024,20:25:14,https://www.scientificamerican.com/article/quantum-computers-can-run-powerful-ai-that-works-like-the-brain/,Quantum Computers Can Now Run Powerful AI That Works like the Brain,1473,"April 22, 2024
5 min read
Quantum Computers Can Now Run Powerful AI That Works like the Brain
The influential AI design that makes chatbots tick now runs on quantum computers
By Rahul Rao
BlackJack3D/Getty Images
Few computer science breakthroughs have done so much in so little time as the artificial intelligence design known as a transformer. A transformer is a form of deep learning‚Äîa machine model based on networks in the brain‚Äîthat researchers at Google first proposed in 2017. Seven years later the transformer, which enables ChatGPT and other chatbots to quickly generate sophisticated outputs in reply to user prompts, is the dynamo powering the ongoing AI boom. As remarkable as this AI design has already proved to be, what if you could run it on a quantum computer?
That might sound like some breathless mash-up proposed by an excitable tech investor. But quantum-computing researchers are now in fact asking this very question out of sheer curiosity and the relentless desire to make computers do new things. A new study published recently in Quantum used simple hardware to show that rudimentary quantum transformers could indeed work, hinting that more developed quantum-AI combinations might solve crucial problems in areas including encryption and chemistry‚Äîat least in theory.
A transformer‚Äôs superpower is its ability to discern which parts of its input are more important than others and how strongly those parts connect. Take the sentence ‚ÄúShe is eating a green apple.‚Äù A transformer could pick out the sentence‚Äôs key words: ‚Äúeating,‚Äù ‚Äúgreen‚Äù and ‚Äúapple.‚Äù Then, based on patterns identified in its training data, it would judge that the action ‚Äúeating‚Äù has little to do with the color ‚Äúgreen‚Äù but a great deal more to do with the object ‚Äúapple.‚Äù Computer scientists call this feature an ‚Äúattention mechanism,‚Äù meaning it pays the most attention to the most important words in a sentence, pixels in an image or proteins in a sequence. The attention mechanism mimics how humans process language, performing a task that is elementary for most young children but that‚Äîuntil the ChatGPT era‚Äîcomputers had struggled with.
If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.
Attention mechanisms currently run on supercomputers with powerful processors, but they still use basic binary bits that hold values of either 0 or 1. Physicists describe these as ‚Äúclassical‚Äù machines, which also include smartphones and PCs. Quantum hardware, on the other hand, taps into the weirdness of quantum mechanics to solve problems too impractical for classical computers. That‚Äôs because quantum bits, aka qubits, can exist as a 0, a 1 or a spectrum of other possible states. So could developers build a superior attention mechanism using qubits? ‚ÄúQuantum computers are not expected to be a computational panacea, but we won‚Äôt know until we try,‚Äù says quantum computing researcher Christopher Ferrie at the University of Technology Sydney, who wasn‚Äôt involved with the new study.
An author of the study, Jonas Landman, had previously crafted quantum facsimiles of other brainlike AI designs to run on quantum hardware. ‚ÄúWe wanted to look at transformers because they seemed to be the state of the art of deep learning,‚Äù says Landman, a quantum computing researcher at the University of Edinburgh and a computing firm called QC Ware. In the new research, he and his colleagues adapted a transformer designed for medical analysis. From a database of images of 1,600 people‚Äôs retinas, some in healthy eyes and some in people with diabetes-induced blindness, the quantum model sorted each image into one of five levels from no damage to the most severe.
Developing their quantum transformer was a three-step process. First, before even touching any quantum hardware, they needed to design a quantum circuit‚Äîa quantum program‚Äôs ‚Äúcode,‚Äù in other words‚Äîfor a transformer. They made three versions, each of which could theoretically pay attention more efficiently than a classical transformer, as demonstrated by mathematical proofs.
Bolstered by confidence from the math, the study authors tested their designs on a quantum simulator‚Äîa qubit emulator that runs on classical hardware. Emulators avoid a problem plaguing today‚Äôs real quantum computers, which are still so sensitive to heat, electromagnetic waves and other interference that qubits can become muddled or entirely useless.
On the simulator, each quantum transformer categorized a set of retinal images with between 50 and 55 percent accuracy‚Äîgreater than the 20 percent accuracy that randomly sorting retinas into one of five categories would have achieved. The 50- to 55-percent range was about the same accuracy level (53 to 56 percent) achieved by two classical transformers with vastly more complex networks.
Only after this could the scientists move on to the third step: operating their transformers on real IBM-made quantum computers, using up to six qubits at a time. The three quantum transformers still performed with between 45 and 55 percent accuracy.
Six qubits is not very many. For a viable quantum transformer to match the chatbot giants of Google‚Äôs Gemini or OpenAI‚Äôs ChatGPT, some researchers think computer scientists would have to create a code that uses hundreds of qubits. Quantum computers of that size already exist, but designing a comparatively colossal quantum transformer isn‚Äôt yet practical because of the interference and potential errors involved. (The researchers tried higher qubit numbers but did not see the same success.)
The group isn‚Äôt alone in its work on transformers. Last year researchers at IBM‚Äôs Thomas J. Watson Research Center proposed a quantum version of a transformer type known as a graph transformer. And in Australia, Ferrie‚Äôs group has designed its own transformer quantum circuit concept. That team is still working on the first step that QC Ware passed: mathematically testing the design before trying it out.
But suppose a reliable quantum computer existed‚Äîone with more than 1,000 qubits and where interference is somehow kept to a minimum. Would a quantum transformer, then, always have the advantage? Maybe not. Head-to-head comparisons between quantum and classical transformers are not the right approach because the two probably have different strengths.
For one thing, classical computers have the benefit of investment and familiarity. Even as quantum-computing technology matures, ‚ÄúIt will take many years for quantum computers to scale up to that regime, and classical computers won‚Äôt stop growing in the meantime,‚Äù says Nathan Killoran, head of software at quantum computing firm Xanadu, who was not involved with the new research. ‚ÄúClassical machine learning is just so powerful and so well financed that it may just not be worth it to replace it entirely with an emerging technology like quantum computing in our lifetimes.‚Äù
Additionally, quantum computers and classical machine learning each excel at different kinds of problems. Modern deep-learning algorithms detect patterns within their training data. It‚Äôs possible that qubits can learn to encode the same patterns, but it is not clear if they are optimal for the task. That‚Äôs because qubits offer the greatest advantage when a problem is ‚Äúunstructured,‚Äù meaning its data have no clear patterns to find in the first place. Imagine trying to find a name in a phone book with no alphabetization or order of any kind; a quantum computer can find that word in the square root of the time a classical computer would take.
But the two options are not exclusive. Many quantum researchers believe a quantum transformer‚Äôs ideal place will be as part of a classical-quantum hybrid system. Quantum computers could handle the trickier problems of chemistry and materials science, while a classical system crunches through volumes of data. Quantum systems might also prove valuable at generating data‚Äîdecrypted cryptographic keys, for example, or the properties of materials that don‚Äôt yet exist, both of which are hard for classical computers to do‚Äîthat could in turn help train classical transformers to perform tasks that now remain largely inaccessible.
And quantum transformers may bring other bonuses. Classical transformers, at the scales they are now being used, consume so much energy that U.S. utilities are keeping carbon-spewing coal plants operational just to meet new data centers‚Äô power demands. The dream of a quantum transformer is also the dream of a leaner, more efficient machine that lightens the energy load.
Rahul Rao is a London-based freelance science writer covering physics, space, technology and their intersections with one another and everything else. He likes snakes, old genre fiction, trains and classic Doctor Who, in no particular order.
Learn and share the most exciting discoveries, innovations and ideas shaping our world today.
Follow Us:
Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers.
¬© 2024 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC.ALL RIGHTS RESERVED."
Science,5/4/2024,20:25:14,https://www.scientificamerican.com/article/is-there-a-human-hiding-behind-that-robot-or-ai/,A Brief History of Automatons That Were Actually People,1221,"April 23, 2024
4 min read
A Brief History of Automatons That Were Actually People
When human labor is hidden under the veneer of a robot or AI tool, that‚Äôs ‚Äúfauxtomation‚Äù
By Brian Contreras
Hernan4429/Getty Images
If you‚Äôve ever asked a chatbot a question and received nonsensical gibberish in reply, you already know that ‚Äúartificial intelligence‚Äù isn‚Äôt always very intelligent.
And sometimes it isn‚Äôt all that artificial either. That‚Äôs one of the lessons from Amazon‚Äôs recent decision to dial back its much-ballyhooed ‚ÄúJust Walk Out‚Äù shopping technology, a seemingly science-fiction-esque software that actually functioned, in no small part, thanks to behind-the-scenes human labor.
This phenomenon is nicknamed ‚Äúfauxtomation‚Äù because it ‚Äúhides the human work and also falsely inflates the value of the ‚Äòautomated‚Äô solution,‚Äù says Irina Raicu, director of the Internet Ethics program at Santa Clara University‚Äôs Markkula Center for Applied Ethics.
If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.
Take Just Walk Out: It promises a seamless retail experience in which customers at Amazon Fresh groceries or third-party stores can grab items from the shelf, get billed automatically and leave without ever needing to check out. But Amazon at one point had more than 1,000 workers in India who trained the Just Walk Out AI model‚Äîand manually reviewed some of its sales‚Äîaccording to an article published last year on the Information, a technology business website.
An anonymous source who‚Äôd worked on the Just Walk Out technology told the outlet that as many as 700 human reviews were needed for every 1,000 customer transactions. Amazon has disputed the Information‚Äôs characterization of its process. A company representative told Scientific American that while Amazon ‚Äúcan‚Äôt disclose numbers,‚Äù Just Walk Out has ‚Äúfar fewer‚Äù workers annotating shopping data than has been reported. In an April 17 blog post, Dilip Kumar, vice president of Amazon Web Services applications, wrote that ‚Äúthis is no different than any other AI system that places a high value on accuracy, where human reviewers are common.‚Äù
News of this technology‚Äôs retirement in U.S. Amazon Fresh stores‚Äîin favor of shopping carts that let customers scan items as they shop‚Äîhas triggered renewed focus on an uncomfortable truth about Silicon Valley hype. Technologies heralded as automating away dull or dangerous work may still need humans in the loop, or as one Bloomberg columnist put it, AI software ‚Äúoften requires armies of human babysitters.‚Äù
It‚Äôs hardly a new phenomenon. Throughout history, canny inventors and entrepreneurs have sought to slap the ‚Äúautomated‚Äù label on what was really just normal human activity. Take the ‚ÄúMechanical Turk,‚Äù a robe-clad robot that inventor Wolfgang von Kempelen debuted in the early 1770s. Von Kempelen would tell observers that the humanoid machine could play full games of chess, opening up the automaton to show off clockwork mechanisms within, as Atlas Obscura recounted.
But the Mechanical Turk was a sham. As many contemporary observers began to suspect, a human operator was hiding in a chamber beneath the chessboard and controlling its movements by candlelight. The clockwork mechanisms were simply window dressing for an easily impressed audience.
Ueber den Schachspieler des Herrn von Kempelen und dessen Nachbildung by Joseph Racknitz, 1789.
Eraza Collection/Alamy Stock Photo
It‚Äôs perhaps fitting that Amazon now runs a platform with the same name that lets companies crowdsource piecemeal online tasks that require human judgment, such as labeling the training data that modern AI systems learn from. After all, charades in the style of the original Mechanical Turk‚Äînominally automated systems that actually rely on human help, or what Jeff Bezos once dubbed ‚Äúartificial artificial intelligence‚Äù‚Äîare common features of the modern Web, where an aura of technological sophistication can sometimes be more important than technological sophistication itself.
‚ÄúThe idea of bringing something inanimate to life is an old and seemingly very human yearning,‚Äù says David Gunkel, a professor of media studies at Northern Illinois University and author of The Machine Question: Critical Perspectives on AI, Robots and Ethics. Pointing to tales as varied as Mary Shelley‚Äôs 1818 novel Frankenstein and the 2014 film Ex Machina, he adds, ‚ÄúIt is in these stories and scenarios that we play the role of God by creating new life out of inanimate matter. And it appears that the desire to actualize this outcome is so persistent and inescapable [that] we are willing to cheat and deceive ourselves in order to make it a reality.‚Äù
Even before products such as ChatGPT and DALL-E kicked off the current artificial intelligence boom, this dynamic played out with less ambitious AI products. Consider X.ai, a company that once touted an automated personal assistant that could schedule meetings and send out e-mails. It turned out that the reason X.ai‚Äôs software seemed so lifelike was that it was literally alive, which Bloomberg reported in 2016: behind the scenes, human trainers were reviewing almost all inbound e-mails. Other concierge and personal assistant programs from that era were similarly human-dependent, and Bloomberg‚Äôs report noted that the draw of venture capital may have incentivized start-ups to frame ordinary workflows as cutting-edge.
It‚Äôs a dynamic that plays out across our increasingly online life. That food delivery robot carting a salad to your front door might actually be a young video game enthusiast piloting it from afar. You might think a social media algorithm is sifting out pornography from cat memes, when in reality human moderators in an office somewhere are making the toughest calls.
‚ÄúThis is not just a question of marketing appeal,‚Äù Raicu says. ‚ÄúIt‚Äôs also a reflection of the current push to bring things to market before they actually work as intended or advertised. Some companies seem to view the ‚Äòhumans inside the machine‚Äô as an interim step while the automation solution improves.‚Äù
In recent months the hype around generative AI has created exciting new opportunities for people to mask workaday human labor under a shiny, PR-friendly veneer of fauxtomation. Earlier this year, for instance, the Internet erupted in a furor over a posthumous George Carlin stand-up special that had purportedly simulated the late comic‚Äôs sense of humor with a machine-learning program trained on his oeuvre. But later, under the threat of a lawsuit from Carlin‚Äôs estate, one of the video‚Äôs creators admitted through a spokesperson that the supposedly AI-generated jokes had in fact been written by an ordinary person.
It was the latest in a centuries-old tradition that continues to enchant and ensnare unwary consumers: humans pretending to be machines pretending to be humans once again.
Brian Contreras is a freelance tech journalist who previously served as the Los Angeles Times' first artificial intelligence reporter. He has written extensively about social media companies, influencers, cryptocurrency, virtual reality and tech regulation. You can reach him with tips and story ideas at briancontreras42 at gmail dot com or @_B_Contreras_¬†on X (formerly Twitter).
Learn and share the most exciting discoveries, innovations and ideas shaping our world today.
Follow Us:
Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers.
¬© 2024 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC.ALL RIGHTS RESERVED."
Science,5/4/2024,20:25:15,https://www.scientificamerican.com/article/a-computer-built-from-dna-can-find-patterns-in-photographs/,A ëComputerí Built from DNA Can Find Patterns in Photographs,712,"April 16, 2024
2 min read
A ‚ÄòComputer‚Äô Built from DNA Can Find Patterns in Photographs
Artificial DNA sorts images like a neural network does
By Allison Parshall
Thomas Fuchs
Brains are the quintessential decision-makers, gathering and weighing information before choosing a path forward. But in the natural world, many simpler systems accomplish similar tasks. Cells use networks of chemical signals to determine when to reproduce or die. Even water could be said to ‚Äúdecide‚Äù whether it will freeze into a snowflake or a hailstone, given the transformation‚Äôs exceedingly complex physics, says Erik Winfree, a molecular computing researcher at the California Institute of Technology.
Winfree has long been intrigued by the physical world‚Äôs hidden information-processing abilities. For a recent study in Nature, he and his collaborators designed a group of artificial DNA strands that, together, can recognize patterns and categorize information. The system bears key similarities to the ‚Äúneural network‚Äù algorithms that underpin many artificial-intelligence models.
To build computerlike circuits with biological machinery, researchers often turn to self-assembling DNA molecules. These customized strands (or ‚Äútiles‚Äù) of DNA, when combined in a test tube and cooled, assemble into predictably shaped mosaics that can convey information.
If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.
The scientists wanted to know whether that type of setup could recognize patterns‚Äîsuch as by sorting grayscale photographs into categories. To represent images in a test tube, the scientists created a code in which each image pixel corresponded to a particular ‚Äúshape‚Äù of DNA tile. The lighter a pixel, the more of its corresponding DNA tile would be present in the solution.
When cooled, the tiles snapped together like a self-assembling jigsaw puzzle into one of three possible shapes, depending on the balance of DNA tile shapes in the mixture. Each shape represented a category, explains co-author Constantine Glen Evans, a molecular computing researcher now at Maynooth University in Ireland.
The system was built to sort 18 photos into three arbitrary categories, but it could also classify images it had never seen before, such as distorted versions of the same pictures. Like a neural network, it recognized general similarities in images ‚Äúrather than looking for an exact match,‚Äù says co-author Arvind Murugan, a physicist at the University of Chicago.
The research is intended not to be an alternative to neural networks themselves but instead to reveal the computational abilities ‚Äúthat matter already has,‚Äù Murugan says. The scientists hope to find similar computational abilities within other systems in nature; such abilities ‚Äúcould be hidden in all kinds of things that we don‚Äôt notice,‚Äù Murugan says.
‚ÄúIt‚Äôs just intrinsically interesting,‚Äù says biomolecular engineer Rebecca Schulman of Johns Hopkins University, who was not involved in the new research. The fact that information can be stored implicitly through the interactions of large groups of molecules, similarly to how it‚Äôs stored in large groups of neurons in a neural network, ‚Äúis something that I have never seen before,‚Äù she says.
The findings are like a first, fleeting glimpse at an ‚Äúexotic‚Äù deep-sea ecosystem, Schulman adds. ‚ÄúIt‚Äôs maybe a calling to go back and look harder.‚Äù
Allison Parshall is a contributing news editor at Scientific American who often covers biology, health, technology and physics. She edits the magazine's Contributors column and has previously edited the Advances section. As a multimedia journalist, Parshall contributes to Scientific American's podcast Science, Quickly. Her work includes a three-part miniseries on music-making artificial intelligence. Her work has also appeared in Quanta Magazine and Inverse. Parshall graduated from New York University's Arthur L. Carter Journalism Institute with a master's degree in science, health and environmental reporting. She has a bachelor's degree in psychology from Georgetown University. Follow Parshall on X (formerly Twitter) @parshallison
Learn and share the most exciting discoveries, innovations and ideas shaping our world today.
Follow Us:
Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers.
¬© 2024 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC.ALL RIGHTS RESERVED."
Science,5/4/2024,20:25:15,https://www.scientificamerican.com/article/smart-gloves-teach-piano-playing-through-touch/,smart gloves teach piano playing through touch,607,"April 16, 2024
2 min read
‚ÄòSmart Gloves‚Äô Teach Piano Playing through Touch
A high-tech pair of gloves can help make learning instruments and other hands-on activities easier
By Riis Williams
Thomas Fuchs
Made of thin cotton and stitched together in only 20 minutes, an experimental pair of gloves isn‚Äôt particularly fashionable or useful for keeping anyone‚Äôs hands warm. Instead the accessory uses tactile sensors woven into its fabric to serve an entirely different purpose: teaching piano and other hands-on skills.
For a study in Nature Communications, Massachusetts Institute of Technology graduate student Yiyue Luo and her colleagues created these ‚Äúsmart gloves‚Äù using haptic technology, which incorporates physical sensations such as vibrations or force to help with tactile activities. Researchers used the gloves to record one pianist‚Äôs hand movements while playing a song. They then relayed those movements to a student through fingertip vibrations, helping the learner build muscle memory and perform the piece with greater precision. ‚ÄúHand-based movements like piano playing are normally really subjective and difficult to record and transfer,‚Äù Luo says. ‚ÄúBut with these gloves we are actually able to track one person‚Äôs touch experience and share it with another person to improve their tactile learning process.‚Äù
Using a computerized embroidery machine, the team embedded small wires linked to a pressure-sensing material in the gloves to detect hand motions. When a piano teacher wearing the gloves repeatedly performed a tune, a machine-learning algorithm processed their movement on the keys and translated it into instructional vibrations. Students wearing their own gloves then attempted to play the same tune, with the fingertip vibrations guiding them through proper movements. (The vibration intensity increased to correct fingering or rhythm mistakes.) By the trials‚Äô end, students who had practiced with the gloves could play more accurately than those who had not. ‚ÄúThis type of learning is sort of like when you‚Äôre first starting to ride a bike,‚Äù says Rice University mechanical engineer Marcia O‚ÄôMalley, who was not involved in the study. ‚ÄúYou use the training wheels to get the sensation of properly riding the bike. Eventually you take them off and start to ride independently.‚Äù
If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.
The team also tested the gloves‚Äô ability to aid people playing online games with a mouse and keyboard, recording motions from experienced players to guide novices. People who gamed with the gloves‚Äô guidance scored better on average than those who did not.
Playing piano or video games is often just for fun, but O‚ÄôMalley adds that with an improved algorithm, coded to identify and capture finer hand movements, the new glove technology could someday help to teach crucial practices such as surgeries. Instructional haptics ‚Äúremove a step in the learning process that auditory and visual learning can‚Äôt,‚Äù she says. ‚ÄúWhen vibration is actually felt directly at the point of action, we can act and learn quicker‚Äîand with that, there‚Äôs so much potential.‚Äù
Riis Williams is a New York City‚Äìbased science journalist who specializes in climate, health and wildlife reporting. She currently serves as Scientific American‚Äôs news intern.
Learn and share the most exciting discoveries, innovations and ideas shaping our world today.
Follow Us:
Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers.
¬© 2024 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC.ALL RIGHTS RESERVED."
Science,5/4/2024,20:25:15,https://www.scientificamerican.com/article/low-earth-orbit-faces-a-spiraling-debris-threat/,low earth orbit faces a spiraling debris threat,1458,"Opinion
April 22, 2024
5 min read
Low-Earth Orbit Faces a Spiraling Debris Threat
Millions of human-made objects travel at high speeds in low-Earth orbit, polluting space and increasing the chance of collision with satellites and other spacecraft
By Aneli Bongers & Jos√© L. Torres
Mark Garlick/Science Photo Library/Getty Images
Space is getting crowded, with junk. Essential satellites delivering navigation, weather forecasts, the Internet and other services face this threat daily. Old rockets, decaying spacecraft and human operations in space leave behind orbital debris that increasingly threatens collisions, menacing a growing space economy. A decade ago the film Gravity dramatized the consequences of space pollution, with an avalanche of space junk sweeping across the sky to batter everything in orbit, including its astronaut hero. We haven‚Äôt done anything serious about it since then.
NASA defines space junk, or orbital debris, as ‚Äúany human-made object in orbit that no longer serves a useful purpose, including spacecraft fragments and retired satellites.‚Äù A 2009 incident, when the U.S. communications satellite Iridium 33 collided with the defunct Russian military satellite Kosmos 2251, serves as a good reminder of its growing threat. That single collision created more than 2,200 pieces of new debris measuring over five centimeters in diameters, according to NASA.
More of these collisions are coming. In February an abandoned Russian satellite passed within about 20 meters of a NASA satellite. SpaceX‚Äôs Starlink satellite constellation alone carried out more than 25,000 collision-avoidance maneuvers from December 2022 to May 2023. And even on Earth, space junk is a problem, with a Florida home struck in March by a battery that fell from an International Space Station cargo mission.
If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.
In space, debris comes from the frequent breakup of expended rocket bodies, explosion of satellites, dead satellites, collisions, paint flakes and even tools lost by astronauts. Around 85 percent of this debris resides within low-Earth orbit, which is below 2,000 kilometers in altitude. NASA estimates this orbit contains around 34,000 pieces of debris larger than 10 cm in diameter, 900,000 objects between 1 cm and 10 cm, and more than 128 million fragments between 1 mm and 1 cm. But let‚Äôs not be fooled by the size. Even small debris traveling at high velocities can trigger catastrophic collisions, with the added problem that fragments smaller than 10 cm are impossible to track with existing surveillance technology. An even more dangerous threat is self-propagation. Called the ‚ÄúKessler syndrome,‚Äù this phenomenon occurs when collisions produce so much debris that Earth‚Äôs orbit becomes unusable for any human activity.
We must stop the growth of space debris, while realizing it is a demanding task. Neither national governments nor international organizations control property rights on orbit, apart from spacecraft ownership. Therefore, space activities are not subject to any centralized regulation or property rights scheme. In outer space ‚Äúfirst come, first serve‚Äù instead applies. Like other global economic failures on Earth (such as fisheries in international waters, high seas sailing and climate change) that international cooperation has failed to solve or left only partly solved, overuse and depletion are direct consequences of such a ‚Äútragedy of the commons.‚Äù
In all these cases, including orbital debris, pollution exerts a cost on society where market prices do not capture the impact. Such ‚Äúexternalities‚Äù are market failures that, in most cases, require intervention from a government or other central authority. Simply put, rocket launch prices don‚Äôt reflect their real costs, which include clean-up expenses. We must reverse this situation before the cost becomes too high. Although human exploration and economic exploitation of outer space are relatively recent (the first human-made spacecraft, Sputnik, successfully launched in 1957), evident market failures and other economic, legal and political issues are arising at rocket speed as commercial, military and scientific activities in outer space expand. SpaceX is now developing a massive rocket that will launch 1.25-ton satellites like a Pez dispenser, adding to a fleet of 5,500 Starlink satellites already in space, part of a planned constellation of 42,000. That‚Äôs particularly alarming, because, according to a study of ours published last year in Ecological Economics, low-Earth orbit can only hold about 72,000 satellites without a real risk of a Kessler syndrome event, under current debris conditions.
LEO stands for low Earth orbit and is the region of space within 2,000 km of the Earth's surface. It is the most concentrated area for orbital debris, represented with white dots and scaled to optimize visibility.
NASA/ODPO
No surprise, spacefaring nations that are most dependent on satellites face the biggest risks of space debris. Instead of cooperating to mitigate space debris, however, they have failed to take decisive action. That‚Äôs despite the increasing probability of losing satellites, resulting in more resources that must be dedicated to debris surveillance tracking and collision-avoidance maneuvers that interrupt services and burn fuel. That, in turn, reduces the operational life of satellites, adding to their costs as the threat burgeons. Nonetheless, spacefaring companies have no incentive to minimize debris generation except for protecting their own spacecraft, which they do with shields.
We need to take both passive and active measures before space junk gets out of control. Space agencies and the United Nations have elaborated guidelines for debris mitigation. Some include changing the design of satellites with shields and reinforcing fuel systems to avoid breakups. Another recommends that all spacefarers provide spacecraft with maneuverability and add reserve fuel to de-orbit derelict spacecraft.
On the active measures side, developing debris-free recovery launch vehicles will greatly help to eliminate the primary source of debris. But there are still other sources, and given current orbiting derelict stock, active depolluting actions are also necessary. That‚Äôs because the life spans of orbital debris vary depending on altitude. Below 200 kilometers, it may only last a few days, whereas those at 1,000 km can last up to a thousand years. At 2,000 km, the debris can remain in orbit for up to 50,000 years without human intervention.
Cleaning space requires designing and implementing active debris removal (ADR) projects. ADR vehicles can be equipped with robotic arms, nets, collecting balloons and other tools. Earth-based lasers might also increase the atmospheric drag of debris, as another option. Policy makers must explore financing the cost of removal policies with instruments already in place for mitigating pollution on Earth. They should also develop guidelines and regulations to share the space junk removal cost among all spacefaring agents.
Finally, there is also a more alarming problem to pay attention to: the military‚Äôs use of space. Space pollution results not only from commercial and scientific activities, but also because of outer space‚Äôs growing strategic value for defense, security and warfare. Hard as it is to understand, Earth's orbit has been polluted not only accidentally but also intentionally, as some countries have conducted antisatellite tests using missiles that destroyed their own satellites. The last one, performed by Russia in 2021, created a vast cloud of hundreds of thousands of fragments, dramatically increasing orbital debris at the most congested and polluted altitudes.
The militarization and weaponization of outer space contribute to orbital debris while acting as a roadblock to the de-pollution of space. Development of debris removal vehicles and devices is hindered by their dual-use status as antisatellite weapons, a significant obstacle to implementing international policies for eliminating orbital pollution. Any ADR technology could also be viewed as an offensive weapon, as it could remove enemy satellites from orbit. For the same reason, the militarization of outer space could threaten the development of new in-space industries for the servicing, refueling, upgrading, maintenance and repair of satellites. Humanity instead needs a clean, safe and regulated space environment to build a better world on Earth.
This is an opinion and analysis article, and the views expressed by the author or authors are not necessarily those of Scientific American.
Aneli Bongers is an associate professor of macroeconomics at the University of M√°laga (Spain). She specializes in environmental economics and circular economy.
Jos√© L. Torres is a professor of macroeconomics at the University of M√°laga (Spain). He has published papers on dynamic macroeconomics and is currently researching various topics related to the economics of outer space.
Learn and share the most exciting discoveries, innovations and ideas shaping our world today.
Follow Us:
Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers.
¬© 2024 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC.ALL RIGHTS RESERVED."
Science,5/4/2024,20:25:15,https://www.scientificamerican.com/article/online-age-verification-laws-privacy/,online age verification laws privacy,1515,"April 16, 2024
6 min read
Online Age Verification Laws Could Do More Harm Than Good
More U.S. states are requiring online ID checks. A proposed French strategy aims to balance child safety with users‚Äô privacy rights
By Lauren Leffer
Leon Neal/Getty Images
It‚Äôs getting a little more onerous to access online pornography. Within the past few weeks, Kansas, Florida and Idaho became the latest U.S. states to pass policies requiring ‚Äúadult‚Äù websites to verify visitors‚Äô age before allowing access. Five other states already had similar statutes, and lawmakers have introduced or are considering them in many more. Across the Atlantic, the European Union has imposed age-verification requirements on three of the world‚Äôs largest pornography sites. France, Germany and the U.K. also recently ordered all such sites to verify that users are aged 18 or older.
Supporters say that the intent of these laws is to prevent children and teenagers from viewing explicit and potentially harmful content. But age-verification requirements have drawn many critics, from the American Civil Liberties Union to Pornhub to online privacy experts. Detractors note that such laws endanger adults‚Äô digital anonymity and put personal data at risk.
Currently the most widely adopted methods of digital age verification involve users sharing sensitive information such as facial scans, official ID cards or banking particulars with third-party companies‚Äîdetails which, in the process, inevitably get linked to individual data on pornography consumption. This poses obvious opportunities for hacking, theft and extortion, argue digital rights advocacy groups, including the Electronic Frontier Foundation.
If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.
Yet there is at least one other approach. Olivier Blazy, a computer scientist and professor at the √âcole Polytechnique in France, worked with the country‚Äôs National Commission on Informatics and Liberty (CNIL) to design a more private system of online age verification in 2022. This method, which has been under evaluation and testing in France since 2023, adds a digital intermediary between a restricted website and an age-verification service. The system prevents the website from ‚Äúseeing‚Äù information that could identify a user. Meanwhile the third-party age verifier cannot detect which site a user is visiting. ‚ÄúThere are still privacy risks,‚Äù Blazy says‚Äîbut ‚Äúit‚Äôs not worse‚Äù than the baseline risk all Internet users accept when they go online from personal computers. Scientific American spoke with Blazy to learn more about the problems of online age verification and how they might be fixed.
[An edited transcript of the interview follows.]
What are the privacy concerns surrounding online age-verification laws?
The privacy concerns come from questions of ‚ÄúWhat kind of data do you collect to do this age verification? And what are you going to do with them?‚Äù The worst example would be if you directly collected people‚Äôs name and the type of website they‚Äôre trying to access. Then someone could establish a list of who follows certain content‚Äîwhich could be used to target groups such as LGBTQ+ people. That would be terrible.
In France recently there have been many big hacks of government websites. The last one targeted the [government] unemployment website, and it affected 43 million people. We know that establishing lists is a very bad idea.
What are the most common strategies suggested for age verification? What vulnerabilities do they have?
One of the solutions people like to imagine is scanning an ID card‚Äîthough we‚Äôve already seen hacks that try to access identity documents. If this happens, it will probably be an easy target for hackers. And it‚Äôs easy to trick these systems. I managed to access a website by downloading a picture of an ID card I found on the Internet.
There‚Äôs also facial recognition‚Äîthat leads to biometric data being stored. Biometric data are also super sensitive, and if this kind of information is leaked, it‚Äôs dangerous. From a hacker‚Äôs point of view, a database of biometric data is like a big pi√±ata that you want to break open because that information provides easy access to do [harmful] things. There is also a lot of bias with facial recognition. It‚Äôs often very bad at estimating the age of women, especially if they are wearing makeup, and people of color.
Something that was classically done on adult websites was to ask users to perform a payment of $0 or ¬£0 just to prove that you have an account that is set up to pay over the Internet. The idea is that these kinds of accounts are only for adults, but that‚Äôs no longer true. Some banks allow these kinds of payments and accounts from people younger than 18. Plus, this is bad because you are telling people to input banking information on a website that is not necessarily super secure. One can also imagine fake porn sites with fake portals that lure people to enter banking information.
What is the alternative strategy you helped develop?
We created an [automated program interface to exist as a firewall] between the content provider and the verification service.... The only information the content provider gets is a yes or no about whether a user is [aged 18 or older]. The only information the age verifier gets is that someone has sent an age-verification request.
In France people already have a digital identity through this portal called FranceConnect. Everybody with a French social security number has an account there. It‚Äôs what you use to pay your taxes. It‚Äôs not the kind of account you‚Äôre going to give away to someone else. We didn‚Äôt work with it directly, but we did try to devise a system that would be compatible with this and still prevent the government from knowing what kind of websites people access.
Our system is run by the user in a browser on their computer or smartphone. It‚Äôs not going to interact with an external server. When a user tries to access a website, the website sends them a challenge, and the user takes that challenge to whatever existing age-verification provider they want to use. The provider gives some kind of digital signature that assures that the user is [aged 18 or older]. The user can then forward that signature to the website.
Are there still privacy risks associated with this protocol?
There are still privacy risks in that sense that any time you use the Internet without a VPN [virtual private network], websites have access to your IP address, so you‚Äôre not anonymous. But what we did is prove that [our system is as secure as] this.
VPNs are often used to bypass age verification or other restrictions. Do these tools make verification laws ineffective?
If you ask me, I‚Äôm against these laws. I don‚Äôt think they‚Äôre super effective. I think teenagers are smart enough to be able to install VPNs.
If you want to defend these kinds of laws, I [do] think it might prevent some young children from accidentally accessing adult content. Maybe young children won‚Äôt think about installing a VPN to purposefully go to such websites. It might raise the age of first [pornography] access, which I think is a good thing, but it will not be efficient. Thinking about the cost of implementing these kinds of solutions and the privacy risk‚Äîit‚Äôs maybe not the best idea.
Are there any other methods to protect children from inappropriate online content?
The nontechnical approach is to educate parents about the dangers of the Internet. When they provide a smartphone to their children, they should understand that [those kids] will be able to talk to strangers [and see and share adult content]. I don‚Äôt want to blame parents; I want to warn parents and have them speak to their children.
If you want to use technology, some forms of parental controls offer a good solution. It‚Äôs not perfect, but it gives an extra layer of protection without the [large-scale] privacy threat.
Considering your stance on France‚Äôs law and similar policies, how does it feel to have developed a privacy protocol to enable age verification?
I really went into it thinking, ‚ÄúHow can I do something that is less bad than what exists?‚Äù It was damage control, knowing [that France will adopt age verification].... I‚Äôm pretty scared of what is going to happen. But at least now we can point to something and say, ‚ÄúLook, we can have some privacy.‚Äù
Lauren Leffer is a contributing writer and former tech reporting fellow at Scientific American. She covers many subjects, including artificial intelligence, climate and weird biology, because she's curious to a fault. Follow her on X @lauren_leffer and on Bluesky @laurenleffer.bsky.social
Learn and share the most exciting discoveries, innovations and ideas shaping our world today.
Follow Us:
Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers.
¬© 2024 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC.ALL RIGHTS RESERVED."
Science,5/4/2024,20:25:16,https://www.scientificamerican.com/article/stanford-ai-index-rapid-progress/,stanford ai index rapid progress,1196,"April 18, 2024
4 min read
AI Report Shows ‚ÄòStartlingly Rapid‚Äô Progress‚ÄîAnd Ballooning Costs
A new report finds that AI matches or outperforms people at tasks such as competitive math and reading comprehension
By Nicola Jones & Nature magazine
xijian/Getty Images
Artificial intelligence (AI) systems, such as the chatbot ChatGPT, have become so advanced that they now very nearly match or exceed human performance in tasks including reading comprehension, image classification and competition-level mathematics, according to a new report. Rapid progress in the development of these systems also means that many common benchmarks and tests for assessing them are quickly becoming obsolete.
These are just a few of the top-line findings from the Artificial Intelligence Index Report 2024, which was published on 15 April by the Institute for Human-Centered Artificial Intelligence at Stanford University in California. The report charts the meteoric progress in machine-learning systems over the past decade.
In particular, the report says, new ways of assessing AI ‚Äî for example, evaluating their performance on complex tasks, such as abstraction and reasoning ‚Äî are more and more necessary. ‚ÄúA decade ago, benchmarks would serve the community for 5‚Äì10 years‚Äù whereas now they often become irrelevant in just a few years, says Nestor Maslej, a social scientist at Stanford and editor-in-chief of the AI Index. ‚ÄúThe pace of gain has been startlingly rapid.‚Äù
If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.
Stanford‚Äôs annual AI Index, first published in 2017, is compiled by a group of academic and industry specialists to assess the field‚Äôs technical capabilities, costs, ethics and more ‚Äî with an eye towards informing researchers, policymakers and the public. This year‚Äôs report, which is more than 400 pages long and was copy-edited and tightened with the aid of AI tools, notes that AI-related regulation in the United States is sharply rising. But the lack of standardized assessments for responsible use of AI makes it difficult to compare systems in terms of the risks that they pose.
The rising use of AI in science is also highlighted in this year‚Äôs edition: for the first time, it dedicates an entire chapter to science applications, highlighting projects including Graph Networks for Materials Exploration (GNoME), a project from Google DeepMind that aims to help chemists discover materials, and GraphCast, another DeepMind tool, which does rapid weather forecasting.
The current AI boom ‚Äî built on neural networks and machine-learning algorithms ‚Äî dates back to the early 2010s. The field has since rapidly expanded. For example, the number of AI coding projects on GitHub, a common platform for sharing code, increased from about 800 in 2011 to 1.8 million last year. And journal publications about AI roughly tripled over this period, the report says.
Much of the cutting-edge work on AI is being done in industry: that sector produced 51 notable machine-learning systems last year, whereas academic researchers contributed 15. ‚ÄúAcademic work is shifting to analysing the models coming out of companies ‚Äî doing a deeper dive into their weaknesses,‚Äù says Raymond Mooney, director of the AI Lab at the University of Texas at Austin, who wasn‚Äôt involved in the report.
That includes developing tougher tests to assess the visual, mathematical and even moral-reasoning capabilities of large language models (LLMs), which power chatbots. One of the latest tests is the Graduate-Level Google-Proof Q&A Benchmark (GPQA), developed last year by a team including machine-learning researcher David Rein at New York University.
The GPQA, consisting of more than 400 multiple-choice questions, is tough: PhD-level scholars could correctly answer questions in their field 65% of the time. The same scholars, when attempting to answer questions outside their field, scored only 34%, despite having access to the Internet during the test (randomly selecting answers would yield a score of 25%). As of last year, AI systems scored about 30‚Äì40%. This year, Rein says, Claude 3 ‚Äî the latest chatbot released by AI company Anthropic, based in San Francisco, California ‚Äî scored about 60%. ‚ÄúThe rate of progress is pretty shocking to a lot of people, me included,‚Äù Rein adds. ‚ÄúIt‚Äôs quite difficult to make a benchmark that survives for more than a few years.‚Äù
As performance is skyrocketing, so are costs. GPT-4 ‚Äî the LLM that powers ChatGPT and that was released in March 2023 by San Francisco-based firm OpenAI ‚Äî reportedly cost US$78 million to train. Google‚Äôs chatbot Gemini Ultra, launched in December, cost $191 million. Many people are concerned about the energy use of these systems, as well as the amount of water needed to cool the data centres that help to run them. ‚ÄúThese systems are impressive, but they‚Äôre also very inefficient,‚Äù Maslej says.
Costs and energy use for AI models are high in large part because one of the main ways to make current systems better is to make them bigger. This means training them on ever-larger stocks of text and images. The AI Index notes that some researchers now worry about running out of training data. Last year, according to the report, the non-profit research institute Epoch projected that we might exhaust supplies of high-quality language data as soon as this year. (However, the institute‚Äôs most recent analysis suggests that 2028 is a better estimate.)
Ethical concerns about how AI is built and used are also mounting. ‚ÄúPeople are way more nervous about AI than ever before, both in the United States and across the globe,‚Äù says Maslej, who sees signs of a growing international divide. ‚ÄúThere are now some countries very excited about AI, and others that are very pessimistic.‚Äù
In the United States, the report notes a steep rise in regulatory interest. In 2016, there was just one US regulation that mentioned AI; last year, there were 25. ‚ÄúAfter 2022, there‚Äôs a massive spike in the number of AI-related bills that have been proposed‚Äù by policymakers, Maslej says.
Regulatory action is increasingly focused on promoting responsible AI use. Although benchmarks are emerging that can score metrics such as an AI tool‚Äôs truthfulness, bias and even likability, not everyone is using the same models, Maslej says, which makes cross-comparisons hard. ‚ÄúThis is a really important topic,‚Äù he says. ‚ÄúWe need to bring the community together on this.‚Äù
This article is reproduced with permission and was first published on April 15, 2024.

Nicola Jones¬†is a contributing editor and writer for Knowable Magazine and lives in Pemberton, British Columbia. Read more about her and her work on her¬†blog.
First published in 1869, Nature is the world's leading multidisciplinary science journal. Nature publishes the finest peer-reviewed research that drives ground-breaking discovery, and is read by thought-leaders and decision-makers around the world.
Learn and share the most exciting discoveries, innovations and ideas shaping our world today.
Follow Us:
Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers.
¬© 2024 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC.ALL RIGHTS RESERVED."
Science,5/4/2024,20:25:16,https://www.scientificamerican.com/article/soviet-era-pseudoscience-lurks-behind-havana-syndrome-worries/,soviet era pseudoscience lurks behind havana syndrome worries,1426,"Opinion
April 24, 2024
5 min read
Soviet-Era Pseudoscience Lurks behind ‚ÄòHavana Syndrome‚Äô Worries
Dodgy studies and fantastic claims have long powered a belief in devious Russian brain weapons, from mind control to microwave devices
By Keith Kloor
Personnel gathered at the U.S. Embassy in Cuba on September 29, 2017 in Havana, Cuba.
Sven Creutzmann/Mambo Photo/Getty Images
In the 1970s U.S. spy agencies believed that Soviet scientists were using telepathy and other supposed paranormal abilities to develop mind-control weaponry. U.S. Army and Air Force journals fretted about a ‚Äúnew mental battlefield‚Äù of ‚Äúpsychic warfare.‚Äù
If you saw The Men Who Stare at Goats, based on the nonfiction book by Jon Ronson, or read Annie Jacobsen‚Äôs Phenomena: The Secret History of the U.S. Government‚Äôs Investigations into Extrasensory Perception and Psychokinesis, then you know what happened next. The Defense Department for the next two decades squandered millions of dollars on ridiculous ‚Äúpsychic spy‚Äù research. It recruited paranormal enthusiasts to practice levitating and walking through walls, among other absurdities.
Most of us now chuckle at this screwball chapter of the cold war annals. But the roots of it, which can be traced to a wellspring of bogus Soviet-era studies fed to visiting American researchers and writers, is worth revisiting to better understand how the U.S. intelligence community got suckered into (and distracted by) a decades-long enchantment with the pseudoscience of parapsychology.
If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.
That history today offers us insight into the origin of Havana syndrome, the controversial medical condition best known for afflicting U.S. diplomats and intelligence officials. Many victims of these ‚Äúanomalous health incidents‚Äù believe they suffer from brain injuries from a secret Russian acoustic or radiological microwave weapon. (Their symptoms include headaches, vertigo and fatigue.) Experts have pointed out why this is scientifically implausible. Moreover, federal investigators have found no trace of such a weapon. And a recent comprehensive National Institutes of Health study revealed no physical evidence of brain damage in self-identified victims‚Äîmany who are former government personnel who worked in the State Department and CIA .
So why does the notion (especially in the news media) still persist that Russia is attacking people with some kind of portable directed energy device? Well, for starters, Russia has boasted of creating such a brain weapon, in various forms over many decades. In the 1990s, American cold warriors pivoted from worrying about Soviet psychic warfare to ‚Äúpsycho-terrorism,‚Äù which they read about in Russian media. These new ‚Äúpsychotronic weapons,‚Äù as the Russians dubbed them, were powered by x-rays, ultrasound and radio waves, and ‚Äúused against the mind to induce hallucinations, sickness, mutations in human cells, ‚Äòzombification,‚Äô or even death,‚Äù wrote military analyst Timothy L. Thomas in the U.S. Army War College‚Äôs quarterly journal Parameters.
If American national security experts took Russia at its word, why shouldn‚Äôt influential Havana syndrome proponents, who frequently cite dated Russian studies on health effects from exposure to microwave radiation?
This includes David Relman, a professor of medicine at Stanford, who in one 2021 NBC interview, points vaguely to this decades-old ‚ÄúRussian literature‚Äù as evidence for a ‚Äúdeliberate‚Äù use of ‚Äúpulsed microwave energy‚Äù in the Havana Syndrome incidents. Relman, notably, headed a controversial 2020 National Academy of Sciences report, which concluded that ‚Äúdirected pulsed RF [radio frequency] energy‚Äù was ‚Äúthe most plausible mechanism‚Äù for Havana syndrome symptoms.
Never mind that the NAS report provided no biological basis for this claim. But it did cite ‚Äúsignificant research in Russia/USSR‚Äù that examined military personnel exposed to pulsed microwave radiation. (Perhaps they were the test subjects for Putin‚Äôs ‚Äúzombie‚Äù ray gun.) These studies supposedly found that individuals suffered dizziness, depression, headaches and fatigue, among other ill effects (all which happen to match up with the symptoms reported by Havana syndrome victims). Oh, and by the way: the NAS report also acknowledges that ‚Äúmany of the studies from the former Soviet Union were flawed in one or more ways.‚Äù
Somehow this dodgy Soviet-era science became the backbone for the microwave weapon theory. Its flimsy rationale defies both common sense and critical scrutiny. And yet, reputable journalism outlets contort themselves to make the argument. The latest example is an April 60 Minutes segment that suggested a dastardly Russian intelligence cell zapped U.S. officials all over the globe, including inside the White House. The 60 Minutes story, as skeptics noted, is fraught with illogical leaps and convenient omissions.
In response, adherents of the Russian microwave weapon theory have pointed to a long and detailed article published by The Insider, an investigative journalistic website that teamed up with 60 Minutes. But if you read the fine print, its story ultimately falls back on‚Äîyou guessed it‚Äîa ‚Äúcorpus of scientific literature‚Äù from the Soviet Union, including a 1974 Soviet patent that claimed to successfully test a device that used radio waves to put people to sleep.
This reminded me of the infamous account of the Soviet psychic who could stop a frog‚Äôs heart with her mind, one of the many fantastical tales of Russian psychic prowess chronicled by American authors in the 1970s. There was a KGB-sponsored pipeline of this stuff that American readers ate up and that seemingly induced the U.S. intelligence community to fall down its paranormal rabbit hole.
As the intelligence historian Filip Kovaƒçeviƒá explained to me, psychic phenomena were at this time incompatible with Marxist (e.g., materialist) ideology in the U.S.S.R. But ESP (extrasensory perception) was all the rage in America‚Äôs Age of the Aquarius, and the Soviets took notice. ‚ÄúThat was something the Westerners wanted to hear (especially in the late 1960s and early 1970s) and the KGB supplied them with it,‚Äù he said to me in an e-mail. ‚ÄúMy sense is that most of what was known about Soviet science in the West during the [cold war] was a result of KGB disinformation.‚Äù
To suggest that the victims of Havana syndrome are just the latest in a long line of KGB dupes would be simplistic and insulting. I don‚Äôt question their sincerity or the pain they have experienced, some still with debilitating injuries. At this juncture, the psychogenic theory strikes me as the most plausible explanation, particularly for those individuals working in a high stress environment, who might also have had earlier injuries. But we can also fault bad journalism, bad science and an inept bureaucracy for the growth of Havana syndrome as a sociocultural phenomenon.
It also seems possible that Havana syndrome is a byproduct of the recently heightened tensions between Russia and the United States that have been building since the 2014 seizure of Crimea that preceded the current war in Ukraine. It‚Äôs worth remembering that the cold war in the 20th century between the two superpowers was rife with mutual paranoia and misinterpretations. In the 2010s, as tensions with China and Russia heightened, the concept of neurowarfare gained traction in military and intelligence circles. The concern here is that America‚Äôs foreign adversaries will develop nonlethal but incapacitating weapons that strike at the brain, via a chemical or biological agent or portable radiological device.
One of the leading Havana syndrome conceptualists is Georgetown University neuroscientist James Giordano, who is adamant that victims were attacked with a sonic or microwave weapon. He has argued this since 2018 and been an influential advisor on the issue to the U.S. government. Today, the idea of neurowarfare is regarded as a fait accompli by up-and-coming thinkers in the military. Havana syndrome‚Äôs emergence and the National Academy of Sciences report‚Äôs conclusions have cinched it.
In the intelligence world, U.S. analysts are taught to be vigilant to cognitive biases that might confirm a preconception. Our adversaries have long sought to exploit such biases as a form of neurowarfare. In the end, that might be the real Havana syndrome.
This is an opinion and analysis article, and the views expressed by the author or authors are not necessarily those of Scientific American.
Keith Kloor is New York City‚Äìbased journalist and adjunct professor of journalism at New York University.
Learn and share the most exciting discoveries, innovations and ideas shaping our world today.
Follow Us:
Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers.
¬© 2024 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC.ALL RIGHTS RESERVED."
Science,5/4/2024,20:25:16,https://www.scientificamerican.com/article/ai-can-transform-the-classroom-just-like-the-calculator/,ai can transform the classroom just like the calculator,1272,"Opinion
April 17, 2024
4 min read
AI Can Transform the Classroom Just Like the Calculator
AI can better education, not threaten it, if we learn some lessons from the adoption of the calculator into the classroom
By Michael M. Crow, Nicole K. Mayberry, Ted Mitchell & Derrick Anderson
Moor Studio/Getty Images
The rapidly expanding use of ChatGPT and other artificial intelligence tools has fired up a fervent debate in academia. On one side of the debate, professors and teachers are concerned over the future of postsecondary learning and threats to traditional disciplines, especially within the humanities, as headlines warn of ‚ÄúThe End of the English Major.‚Äù
Nevertheless, AI is here and about a third of teachers, from kindergarten through high school, report using it in the classroom, according to a recent survey. While many of our colleagues in higher education policy, science policy, and university design criticize or dismiss generative AI, we are instead decidedly optimistic it will follow a pattern seen in other technologies that have enhanced educational access and success. We believe that when new technologies are embraced, core aspects of learning, including curriculum, instruction and assessment, can be revolutionized. We are optimistic about AI, but we don‚Äôt see it as a hero. Students and instructors are still the heroes of human learning, even when AI is involved.
History supports this view. From the Gutenberg press to online math classes, technologies that improve access to quality learning opportunities are routinely dismissed by critics and skeptics, especially by those who hold the reins in the classroom.
If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.
Consider the calculator. A survey in the mid-1970s carried out by Mathematics Teacher magazine found that 72 percent of respondents‚Äîmainly teachers and mathematicians‚Äîopposed equipping seventh graders with calculators. Highlighted in 1975 in Science News, this survey mirrored the broader discourse of the Sesame Street era concerning the introduction of calculators into classrooms, just when costs were approaching the point that some schools could afford to have up to one calculator per student.
Calculators met resistance from educators who feared an overdependence on technology would erode students‚Äô math skills. As one professor observed of students and calculators, ‚ÄúI have yet to be convinced that handing them a machine and teaching them how to push the button is the right approach. What do they do when the battery runs out?‚Äù
It is easy to see how the case of the calculator mirrors current concerns about generative AI. The College Board made a similar argument in an article published last spring that mused about the ‚ÄúGreat Calculator Panic of the 1980s and ‚Äò90s.‚Äù Critics of AI in the classroom argue that students might never learn to writeor respond to written prompts independently if they can simply ask an AI to do it for them. The hypothetical scenario where the Internet or servers are down raises fears that students would be unable to write a simple sentence or compose a basic five-paragraph essay.
Narrow arguments over essay integrity and potential declines in learning quality miss the broader perspective on how this technology could positively reshape curriculum, instruction and assessment.
In classrooms, technology, curriculum, instruction, and assessment evolve together to reshape education. We see this historically with calculators and are now witnessing it unfold in real time with the emergence of generative AI tools.
The introduction of calculators into classrooms didn't set in motion the demise of mathematics education; instead, it significantly broadened its scope while inspiring educators and academics to rethink the educational limits of mathematics. This shift fostered a climate ripe for innovation. Looking at today‚Äôs math landscape and what existed in the 1970s, we would be hard-pressed to consider the past superior to the present, to say nothing of the future. Today, high school students use (and more importantly, comprehend) graphing calculators and computers better than undergraduate engineering students in university labs could only a generation ago. Today‚Äôs math learning environment is observably more dynamic, inclusive and creative than it was before ubiquitous access to calculators.
In a parallel vein, generative AI promises to extend this kind of innovation in critical thinking and the humanities, making it easier for students to grasp foundational concepts and explore advanced topics with confidence. AI could allow for customized learner support‚Äîadapting to the individual pace and learning style of each student, helping to make education more inclusive and tailored to specific needs. Generative AI can better the humanities by making reading and writing more accessible to diverse students, including those with learning disabilities or challenges with traditional writing methods.
Just as calculators led us to reevaluate legacy teaching methods and embrace more effective pedagogical approaches, generative AI calls for a similar transformation in how we approach assignments, conduct classes and assess learning. It will shift us from viewing the college essay as the pinnacle of learning to embracing wider creative and analytical exercises, ones facilitated by AI tools.
The successful integration of calculators into math education serves as a blueprint for the adoption of generative AI across the curriculum. By designing assignments with the expectation that generative AI will enhance rather than shortcut them, educators can foster learning that values creativity, critical thinking and efficient study. This shift necessitates a broader, more adaptable approach to teaching and learning, one that recognizes the potential of technology to elevate educational standards and broaden access to knowledge.
This history points to broader questions over the efficiency and fairness of long-standing educational mechanisms. Take, for example, college admissions essays, which are known to perpetuate bias in university admissions. What if AI allowed us to reconceptualize the tools for students to demonstrate their aptitude and college preparedness? What if AI could allow students to match their intended college major more accurately to the most supportive and corresponding place of higher learning? In academia, we shouldn‚Äôt focus solely on AI‚Äôs potential for misuse but also on its capability to revolutionize curricula and approaches to learning and teaching.
Far from fearing technological progress, history teaches us to embrace it to broaden and democratize learning. The greater challenge lies not in resisting change, but in leveraging these innovations to develop curricula that address the needs of all learners, paving the way for a more equal and effective education for everyone. Looking ahead, generative AI is not so much a problem to be solved, but instead a powerful ally in our efforts to make education meaningfully universal.
This is an opinion and analysis article, and the views expressed by the author or authors are not necessarily those of Scientific American.
Michael M. Crow is the president of Arizona State University and a Foundation Professor of science and technology policy.
Nicole K. Mayberry is an assistant research professor at Arizona State University‚Äôs School of Public Affairs.
Ted Mitchell is the president of the American Council on Education and a former U.S. undersecretary of education.
Derrick Anderson is the senior vice president for education futures at the American Council on Education and an associate professor of science and technology policy at Arizona State University.
Learn and share the most exciting discoveries, innovations and ideas shaping our world today.
Follow Us:
Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers.
¬© 2024 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC.ALL RIGHTS RESERVED."
Science,5/4/2024,20:25:16,https://www.scientificamerican.com/article/what-philosopher-ibn-sina-can-teach-us-about-ai/,what philosopher ibn-e-sina can teach us about ai,1347,"Opinion
April 18, 2024
5 min read
What Philosopher Ibn Sina Can Teach Us about AI
A philosopher who lived centuries before artificial intelligence might be able to help us understand the field's personhood questions
By Abigail Tulenko
Islamic philosopher Ibn Sina (980-1037 C.E.)
Mohamed Osama/Alamy Stock Photo
In 2022, Google engineer Blake Lemoine developed a rapport with an excellent conversationalist. She was witty, insightful, and curious; their dialogues flowed naturally, on topics ranging from philosophy to TV to dreams for the future. There was just one problem: she was an AI chatbot.
In a series of conversations with Google‚Äôs LaMDA language model, Lemoine became gradually convinced the chatbot was a personlike you or me. ‚ÄúI know a person when I talk to it,‚Äù he told the Washington Post in 2022.
Whether you believe Lemoine‚Äôs claims or not, the question emerges: Do we, in fact, know a person when we talk to one?
If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.
Personhood generally refers to a moral status; in most ethical frameworks, particular considerations‚Äîrights, duties, praise, blame, dignity, agency‚Äîemerge at the level of the person. So the question of whether electronic systems do or could merit the status of personhood has wide-ranging implications for how we engage with these technologies.
To assess the possibility of e-personhood, we need some standard of personhood generally. In recent years many philosophers have argued that what makes us persons is our capacity for conscious experience. But how do we define consciousness? What external evidence can we use to determine whether a being is conscious?
The lack of consensus on these questions is one of the reasons that the debate around AI personhood has long been at a bit of a stalemate. The question emerges: What other criteria do we have for assessing the possibility of e-personhood? As a doctoral candidate studying the philosophy of science, I believe a path toward answering this futuristic question may lie in our distant past‚Äîin the work of the early Islamic philosopher Ibn Sina (980‚Äì1037 C.E.).
Ibn Sina lived centuries before the invention of the printing press, much less artificial intelligence. And yet he was concerned with many of the same questions AI ethicists think about today‚Äîquestions like: What makes a person a person, as opposed to an animal?
Just as contemporary AI researchers are interested in comparing the processes that underpin human and AI responses to similar tasks, Ibn Sina was interested in comparing the internal processes humans and animals might undergo to arrive at similar behavior outputs. For him, one key distinguishing capacity of the human person is the capacity to grasp ‚Äúthe universal.‚Äù Whereas animals can only think about particulars (the specific things right in front of them), humans can reason from generalized rules.
In Al-Nafs, Ibn Sina discusses a popular ancient example of a sheep perceiving a wolf. Whereas a human being would invoke a broad principle‚Äî‚ÄúWolves are generally dangerous, and this particular animal in front of me is a wolf; therefore, I should run away‚Äù‚Äîhe claims that animals think differently. They don‚Äôt reason from a rule; they just see the wolf and know to run. They are limited to ‚Äúparticulars‚Äù‚Äîthat wolf‚Äîrather than reasoning about universal qualities of wolves.
The distinction Ibn Sina draws between human and animal psychology bears strong resemblance to a distinction that contemporary computer scientists are investigating with regard to AI. Current research suggests that artificial neural networks lack the ability for systematic compositional generalizability. Linguists and cognitive scientists use this term to describe the types of inferences we make from generalized rules. It‚Äôs widely assumed to be one of the primary ways humans reason in everyday life. Whereas humans abstract meanings from sequences of words that they can then combine into more complex ideas, AI fishes within statistical datasets for specific data entries that match the particular task at hand.
This difference explains a great deal about the limitations of contemporary AI. To see it at play, look to the ubiquitous CAPTCHA tests used to distinguish between humans and bots. ‚ÄúLook at these curvy letters. Much curvier than most letters, wouldn‚Äôt you say? No robot could ever read these,‚Äù comedian John Mulaney quips in a 2018 Netflix special. It seems absurd, but it‚Äôs true; sufficient alterations make it difficult for even the most sophisticated artificial systems to recognize letters. This is because these systems lack the compositional capacity to make abstract generalizations about the core features of a given letter and apply it to a particular warped example.
This difference between human and artificial cognition maps neatly to Ibn Sina‚Äôs description of what is unique about human reasoning. In al-≈†hifƒÅ, he describes how ‚Äúthe intellect ‚Ä¶ learns what things are shared in common and what things are not, and so extracts the natures of things common in species.‚Äù On his account, humans extract the essential features of things from their less-essential features to form generalized concepts. We then reason using these concepts, applying them to cases.
For example, as children we learn to extract a core feature of the letter X: its being composed of two crossed lines. We then abstract‚Äîmake a universal generalization about the core features of an X‚Äîto conclude that all Xs are composed of two crossed lines. Finally, by applying this generalization, we can recognize specific Xs. We know that two crossed lines are a core feature of the letter X and that the random additional lines and warping in the CAPTCHA image are not.
The computer, in contrast, is unable to deduce that this image represents an X unless it has been fed an exact image of an X (or something sufficiently similar). The additional lines and warped shape are enough to make this X unrecognizable, because it does not match the computer‚Äôs large repository of specific images categorized as Xs.
Similarly, if an artificial neural network were presented with the task of the sheep, it would not reason as the human does, from a general concept of wolf-ness to features of the particular wolf such as dangerousness. Instead, it would reason as the sheep does, constrained to the realm of particulars.
A crucial difference between the sheep and the artificial neural network is that the artificial neural network has access to a much larger repository of particulars in the form of increasingly exhaustive datasets. What makes deep learning so successful at language tasks is its access to large datasets of vast numbers of particulars, rather than the genuine replication of human reasoning through compositional generalizability.
Ibn Sina‚Äôs core criterion for personhood‚Äîreasoning from universals‚Äîclosely resembles systematic compositional generalizability. This criterion could provide a potentially testable standard for personhood. In fact, so far, AI has failed this test in numerous studies. Whether or not one adopts it as a solution, Ibn Sina‚Äôs account provides a new lens on the problem of personhood that challenges the assumptions of consciousness-centered accounts.
Scientific ethics is so often concerned with the cutting edge‚Äîthe latest research, the newest technology, a constant influx of data. But sometimes the questions of the future require careful consideration of the past. Looking to history allows us to look beyond the preoccupations and assumptions of our time and may just provide refreshing approaches toward current stalemates.
This is an opinion and analysis article, and the views expressed by the author or authors are not necessarily those of Scientific American.
Abigail Tulenko is a Ph.D. student at Harvard University, studying the philosophy of science. More of her work can be found at her website: abigailtulenko.com
Learn and share the most exciting discoveries, innovations and ideas shaping our world today.
Follow Us:
Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers.
¬© 2024 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC.ALL RIGHTS RESERVED."
Science,5/4/2024,20:25:17,https://www.scientificamerican.com/podcast/episode/how-a-new-ai-model-helps-volcanic-history-rise-from-the-ashes/,how a new ai model helps volcanic history rise from the ashes,1301,"April 17, 2024
How a New AI Model Helps Volcanic History Rise from the Ashes
Volcano detectives use artificial intelligence to sleuth out ancient secrets in Alaska.
By Emily Schwing
Smithsonian/Getty Images
Emily Schwing: In 1912 a volcano in Alaska more than blew its top. Known as Novarupta, it was responsible for the most powerful volcanic eruption of the 20th century.
Kristi Wallace: The reason we care about the big eruptions [is] because they‚Äôre also the ones that scale to the biggest hazards and biggest impacts, both locally and regionally‚Äîsometimes globally.
Schwing: Kristi Wallace is a research geologist with the U.S. Geological Survey at the Alaska Volcano Observatory.
If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.
Wallace: We often don‚Äôt have access to the deep history, the old history. We only have the last 10,000 or 12,000 years; glaciers wiped everything else away. And so, because we can‚Äôt study those, we can‚Äôt really speak to the biggest eruptions through the history of the volcano.
Schwing: Now scientists believe they‚Äôve found a way to sleuth out the deep history of volcanoes such as Novarupta, and they‚Äôre doing it with artificial intelligence.
You‚Äôre listening to Scientific American‚Äôs Science, Quickly. I‚Äôm Emily Schwing.
[Clip: Show theme music]
Schwing: Novarupta dramatically reworked the surrounding landscape and buried nearby communities in more than two feet of ash. Novarupta is a volcano six miles away from another powerful volcano, Mount Katmai. And this wasn‚Äôt the only time one of these volcanoes proved itself a powerful and dramatic volcanic force.
Jordan Lubbers: When we applied our model to 800,000 years of ash layers in the Gulf of Alaska, we saw that Katmai was responsible for producing a lot of the eruption deposits in marine cores going back to 800,000 years, and they looked almost identical geochemically to the 1912 eruption.
Schwing: Jordan Lubbers is a third-year postdoctoral fellow and geologist with the U.S. Geological Survey. And those ash layers he‚Äôs talking about‚Äîthey‚Äôre called tephras.
Lubbers: So we have a lot of tephra layers in the Gulf of Alaska that we don‚Äôt know where they came from.
Schwing: Jordan says tephras are like a volcano‚Äôs fingerprint.
Lubbers: So can we, can we identify a way that these volcanoes all have their own unique kind of geochemical fingerprint and then use that fingerprint to apply to ash layers that we don‚Äôt know the source volcano to kind of better reconstruct a long-term volcanic history of Alaska and the surrounding region?
Schwing: There have been lots of large tephra-producing eruptions in Alaska and northwestern Canada. But they‚Äôre hard to get to.
Lubbers: A lot of the locations that we‚Äôre going to to research these volcanoes are not accessible by road, which means either plane or helicopter travel or something like that. So the fieldwork is, is quite challenging, and so I guess that makes us somewhat sample-limited in that we don‚Äôt, we don‚Äôt have as much information as maybe we would like from these volcanoes because we don‚Äôt have a lot of time to go for boots on the ground at these volcanoes to learn about them.
Schwing: And there‚Äôs another challenge: when glaciers started to retreat at the end of the last glacial maximum, they wiped away evidence of large tephra-producing eruptions in the Far North.
You can think of Jordan and his colleague Kristi as volcano detectives. They are trying to link tephras with their source volcanoes. Now they‚Äôre using machine learning - AI‚Äîas their magnifying glass. They‚Äôve created a model that uses existing data from tephras.
Wallace: And then [we] use this machine-learning technique to say, ‚ÄúOkay, this is what‚Äôs erupted in the last 10,000 years. Can you train the model to tell us where these much older deposits come from?‚Äù Hundreds of thousands of years old, potentially. ‚ÄúDo they link back to these same volcanoes?‚Äù And then we can go back to that‚Äîlet‚Äôs say Katmai or any one of our 130 volcanoes that we‚Äôre dealing with‚Äîcan we then really tell the true story of the volcano?
Schwing: Their findings were published in November 2023 in the journal Geochemistry, Geophysics, Geosystems.
Jordan says using AI also means they have a more accurate way to differentiate among individual volcanoes.
Lubbers: A lot of, a lot of these volcanoes are quite similar. And the human eye or, like, classical statistics might confuse them and mix these things up. But what machine learning basically allows us to do is kind of think about things in many more dimensions, and we give it the data from all these volcanoes, and then we give it the answers that we know‚Äîlike we collected this sample from this volcano, so we know that that‚Äôs where it erupted from. Now come up with a set of rules to then tell us what makes all these volcanoes unique. And that‚Äôs really the power here, is that it‚Äôs able to better classify unknown eruptions.
Schwing: Both Jordan and Kristi say they will continue training their model to strengthen its accuracy and to better understand AI-generated answers to their source volcano inquiries.
Jordan says work like this is useful not only in Alaska and the surrounding region, but anywhere volcanic activity is a known threat to public safety.
Lubbers: All volcanic arcs are slightly different. And so this is what a machine-learning model would figure out. It would figure out maybe a new set of rules that‚Äôs different for the Andes, that‚Äôs different for Sumatra, that‚Äôs different for Kamchatka, that‚Äôs different for Italy. But with proper data handling and, you know, good, good sorts of logical tests and expert, expert kind of opinions from geologists in that area, there‚Äôs no reason that this can't be applied elsewhere.
Schwing: And Kristi says the new tool could also be useful across disciplines in the scientific research community.
Wallace: When you‚Äôre looking at tephra deposits, they represent an isochron, so a single time in history. It‚Äôs instantaneous. It falls to the ground, and it tells you time, it gives you a sense [that] something happened at a specific time, and basically they link marine records [and] lacustrine records in lakes with terrestrial records. And so that helps you to understand, you know, ice ages and climactic events or archaeological time frames or when people were moving around. There‚Äôs all sorts of context that it could put things into.
Schwing: And that context could be key to a new understanding of how massive eruptions like the one at Novarupta changed life for the people who lived through them.
Wallace: This is the first time that the geochemical data has been published. And so now people who are working all across the state, even globally, have access to these data, and they can now correlate the tephras back to a location and an age. That‚Äôs really, really powerful.
Schwing: Maybe even as powerful as the volcanic eruptions themselves.
[Clip: Show theme music]
Scientific American‚Äôs Science, Quickly is produced by Rachel Feltman, Kelso Harper, Carin Leong, Madison Goldberg and Jeff DelViscio and edited by Madison Goldberg, Elah Feder, Alexa Lim and Anaissa Ruiz Tejada, with fact-checking by Shayna Posses and Aaron Shattuck. Our theme music was composed by Dominic Smith.
For Science, Quickly, I‚Äôm Emily Schwing.
Learn and share the most exciting discoveries, innovations and ideas shaping our world today.
Follow Us:
Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers.
¬© 2024 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC.ALL RIGHTS RESERVED."
Science,5/4/2024,20:25:17,https://www.scientificamerican.com/article/spiderlike-mars-robot-reachbot/,spider like mars robot reachbot,1072,"April 17, 2024
4 min read
Spiderlike Mars Robot Might One Day Crawl through Unexplored Volcanic Caves
This eight-legged probe would scour Mars‚Äôs underground lava tubes for places where explorers might camp‚Äîor for signs of past life
By Lauren Leffer
Field testing the ReachBot in a lava tube in the Mojave Desert.
BDML Stanford University
A robot designed in the spiderlike image of a ‚Äúdaddy longlegs‚Äù may be on its way to help scientists explore the next frontier on Mars: caves. A partial prototype of the machine, called ReachBot, has demonstrated its ability to grasp the uneven walls of a natural cavern in California‚Äôs Mojave Desert, as described in a study published Wednesday in Science Robotics.
The tests show that ReachBot‚Äôs unique gripping mechanism and arachnid configuration may be useful for extraterrestrial spelunking. ‚ÄúIf you really want to explore all over the inside of a [Martian] cave, ReachBot is going to be hard to beat,‚Äù says senior study author Mark Cutkosky, a mechanical engineering professor at Stanford University.
Six rovers have already successfully landed on Mars, but much of the planet remains unexplored‚Äîincluding its caves and lava tubes, underground corridors formed by molten rock. ‚ÄúThe subsurface has never really been examined, other than seismic measurements and penetrating radar,‚Äù says Wolfgang Fink, an associate professor of engineering and Edward and Maria Keonijan Endowed Chair at the University of Arizona, who was not involved in the ReachBot research. Some astrobiologists think this extensive subterranean network could be the Martian locale most likely to harbor signatures of past or present alien life. Caves might also be potential sites for human habitation.
If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.
Navigating irregular, unmapped and descending terrain 140 million miles from Earth would clearly require some highly specialized equipment. Other robots proposed for Martian cave exploration include the caninelike, four-legged NeBula-SPOT and the Descent and Exploration in Deep Autonomy of Lava Underground Structures (DEADALUS) device, which would hang into a pit on a tether from a surface rig. A grasping machine such as ReachBot, Fink says, might offer a few advantages‚Äîsuch as the ability to access walls and ceiling areas beyond the reach of ground-based rovers or short-range flying drones. ReachBot‚Äôs strong limbs could also potentially drill core samples and execute other forceful tasks.
To build something that can traverse a large area and still be lightweight and maneuverable, Cutkosky and his colleagues found inspiration in arachnids‚Äîspecifically, long-legged spider cousins called harvestmen, aka daddy longlegs. ReachBot has a small central body and up to eight appendages. Those limbs, or booms, can be rigid or rolled up, like a metal measuring tape. (In small-scale lab tests, the engineers used ordinary tape measures as stand-ins for the much more expensive carbon-fiber material that would be used in space.)
At the end of each boom is a three-fingered gripper equipped with ‚Äúmicrospines,‚Äù made from sewing needles, that help the bot firmly grasp rough rock. These grippers stabilize the robot by maintaining uniform tension across each boom, a function Cutkosky likens to ‚Äúspokes on a bicycle wheel.‚Äù Cameras and sensors on the central body and the grippers enable ReachBot to perceive its surroundings and choose where to place its mechanical hands. The slow-moving bot is designed to advance methodically by detaching and relocating one boom at a time, while the central body shifts position as booms extend and retract.
‚ÄúFrom a pure robotics point of view, this is a really great idea,‚Äù Fink says. He has reservations, though, about how a robot as complex as ReachBot might hold up in a Martian cave, where a lot can go wrong and repairs are difficult. ‚ÄúOne would almost prefer something simpler,‚Äù he says, pointing to the beach ball‚Äìesque Tumbleweed rover concept design from NASA‚Äôs Jet Propulsion Laboratory.
Minimizing fragility and managing the moving parts are ongoing concerns, Cutkosky says, but he notes that some protective redundancy is built into ReachBot; one or two damaged limbs would not prevent it from functioning. Michelle Rosen, an assistant professor of mechanical engineering at the Cooper Union for the Advancement of Science and Art and a robotics researcher uninvolved in the new study, also sees the value of eight appendages. ‚ÄúRedundancy like that is a really good thing, especially when you‚Äôre thinking about planetary exploration,"" she says, adding that the proof-of-concept work was ‚Äúsuper compelling.‚Äù
ReachBot's three-fingered gripper firmly grasps handholds in cave walls.
BDML Stanford University
In 2023 lead study author Tony Chen, a robotics expert at Stanford University, hiked through the Mojave Desert with some of his co-researchers to a lava tube chosen for its likely similarities to Martian caves. There they ran tests with a single-boom version of ReachBot mounted on a tripod to see how well it could identify grip points on the lava tube‚Äôs walls. The perception system ‚Äúworked really well‚Äù to home in on suitable robot handholds, Chen says. And the bot‚Äôs boom and gripper mechanism performed as hoped, reaching for those points with a secure grip.
But one thing the team hadn‚Äôt planned for was the sheer volume of magnetic dust present inside the lava tube. The ferrous crud didn‚Äôt disable ReachBot during three days of testing, but ‚Äúso much dust accumulated inside the robot‚Äù that this may pose a longer-term problem, Chen notes. Future designs will have to deal with such intrusions, especially if a machine like ReachBot is destined for a distant, dusty world. ‚ÄúWe‚Äôre still quite far away from actually being on Mars,‚Äù Chen says. It‚Äôs one small grasp for a robot prototype on Earth, hinting at a leap in our knowledge of what lies beneath other planets.
Lauren Leffer is a contributing writer and former tech reporting fellow at Scientific American. She covers many subjects, including artificial intelligence, climate and weird biology, because she's curious to a fault. Follow her on X @lauren_leffer and on Bluesky @laurenleffer.bsky.social
Learn and share the most exciting discoveries, innovations and ideas shaping our world today.
Follow Us:
Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers.
¬© 2024 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC.ALL RIGHTS RESERVED."
